{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "id": "KrRe5G3H-jTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "### Q Network & Some 'hyperparameters'\n",
        "\n",
        "QNetwork1:\n",
        "Input Layer - 4 nodes (State Shape) \\\n",
        "Hidden Layer 1 - 64 nodes \\\n",
        "Hidden Layer 2 - 64 nodes \\\n",
        "Output Layer - 2 nodes (Action Space) \\\n",
        "Optimizer - zero_grad()\n",
        "\n",
        "QNetwork2: Feel free to experiment more\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''\n",
        "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
        "'''\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64        # minibatch size\n",
        "GAMMA = 0.99           # discount factor\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork2(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "            fc3_units (int): Number of nodes in third hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork2, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, fc2_units)\n",
        "        self.register_full_backward_hook4 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc3(F.relu(self.fc2(x))))\n",
        "\n",
        "        return self.register_full_backward_hook4(x)"
      ],
      "metadata": {
        "id": "r7DP7Y_i_8PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "### Replay Buffer:\n",
        "\n",
        "This is a 'deque' that helps us store experiences. Recall why we use such a technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3mNBpDOCSWd"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        print(action_values)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        #Epsilon-greedy action selection (Already Present)\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "          return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Team():\n",
        "\n",
        "  def __init__(self,self_features,opponent_features):\n",
        "    self.agent = Agent(2,6,0)\n",
        "    self.self_features = self_features\n",
        "    self.opponent_features = opponent_features\n",
        "    self.explore_wicket = 0\n",
        "    self.explore_runs = 0\n",
        "\n",
        "  def get_next_batter(self,order,wicket):\n",
        "    order = {k:v.tolist() for k,v in order.items()}\n",
        "    order = dict(sorted(order.items(), key=lambda item: item[wicket]))\n",
        "    next_batter = list(order)[-1]\n",
        "    del order[next_batter]\n",
        "    return next_batter\n",
        "\n",
        "  def get_next_bowler(self):\n",
        "    return np.random.randint(0,5)\n",
        "\n",
        "  def get_batting_action(self, ball, total_runs, wickets_left, score_to_chase, feature_batter, feature_bowler): # code for UCTS\n",
        "    # batting_action = np.random.randint(0,6)\n",
        "    state = np.array([ball, wickets_left])\n",
        "    return self.agent.act(state)\n",
        "\n",
        "  def get_bowling_action(self,ball,total_runs,wickets_left,score_to_chase,feature_batter,feature_bowler): # code for UCTS\n",
        "    bowling_action = np.random.randint(0,3)\n",
        "    return bowling_action\n",
        "\n",
        "  def get_explore_actions(self):\n",
        "    batter = np.random.randint(0,5)\n",
        "    bowler = np.random.randint(0,5)\n",
        "    batting_action = np.random.randint(0,6)\n",
        "    bowling_action = np.random.randint(0,3)\n",
        "    feature_batter = self.self_features[batter, 0:2]\n",
        "    feature_bowler = self.self_features[bowler, 2:4]\n",
        "    return feature_batter, feature_bowler, batting_action, bowling_action, batter, bowler\n",
        "\n",
        "  def set_explore_outcomes(self,wicket,runs):\n",
        "    self.explore_wicket = wicket\n",
        "    self.explore_runs = runs\n",
        "\n",
        "  def explore_compute(self):\n",
        "    pass\n",
        "\n",
        "  def explore_dp(self):\n",
        "    pass\n",
        "\n",
        "  # def call_dqn(self):\n",
        "  #   agent = Agent(state_size= 3, action_size = 6, seed = 0)\n",
        "  #   scores = self.dqn()\n",
        "\n",
        "class Australia(Team):\n",
        "  pass\n",
        "\n",
        "class India(Team):\n",
        "  pass"
      ],
      "metadata": {
        "id": "8lKtKg2kelA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pout_actions_min = np.array([0.005,0.01,0.02,0.1,0.2,0.3])\n",
        "# pout_actions_max = np.array([0.05,0.1,0.15,0.25,0.35,0.45])\n",
        "# prun_features_max = 0.65\n",
        "# prun_features_min = 0.25\n",
        "\n",
        "# risk_max_aggression = 1.2\n",
        "# risk_min_aggression = 1.05\n",
        "# risk_max_economical = 0.8\n",
        "# risk_min_economical = 0.5\n",
        "\n",
        "# easy_max_aggression = 0.2\n",
        "# easy_min_aggression = 0.3\n",
        "# easy_max_economical = -0.2\n",
        "# easy_min_economical = -0.05\n",
        "\n",
        "\n",
        "# batting_action_runs_map = np.array([0,1,2,3,4,6])\n",
        "# bowling_action_risk_map = np.array([-0.5,0,1])\n",
        "\n",
        "# class Environment:\n",
        "#     def get_outcome_random_env(self,feature_batter, feature_bowler, batting_action, bowling_action):\n",
        "#       wicket = np.random.randint(0,2)\n",
        "#       runs = np.random.randint(0,7)\n",
        "#       return wicket, runs\n",
        "#     def get_outcome_env(self,feature_batter, feature_bowler, batting_action, bowling_action):\n",
        "#       # wicket = np.random.randint(0,1)\n",
        "#       # runs = np.random.randint(0,7)\n",
        "#       runs = 0\n",
        "#       pout = pout_actions_min[batting_action]*(1-((feature_batter[0]-1)/4))+((feature_batter[0]-1)/4)*pout_actions_max[batting_action]\n",
        "#       risk = 1\n",
        "#       if (bowling_action == 2):\n",
        "#         risk = risk_max_aggression*(1-((feature_bowler[0]-1)/4))+((feature_bowler[0]-1)/4)*risk_min_aggression\n",
        "#       if (bowling_action == 0):\n",
        "#         risk = risk_max_economical*(1-((feature_bowler[0]-1)/4))+((feature_bowler[0]-1)/4)*risk_min_economical\n",
        "#       pout = pout*risk\n",
        "#       wicket = np.random.choice(2,1,p=[1-pout,pout])\n",
        "#       if (wicket == 0):\n",
        "#         prun = prun_features_max*(1-((feature_batter[1]-1)/4))+((feature_batter[1]-1)/4)*prun_features_min\n",
        "#         easy = 0\n",
        "#         if (bowling_action == 2):\n",
        "#           easy = easy_max_aggression*(1-((feature_bowler[1]-1)/4))+((feature_bowler[1]-1)/4)*easy_min_aggression\n",
        "#         if (bowling_action == 0):\n",
        "#           easy = easy_max_economical*(1-((feature_bowler[1]-1)/4))+((feature_bowler[1]-1)/4)*easy_min_economical\n",
        "#         # print(prun, feature_batter[1],easy)\n",
        "#         prun = prun + easy\n",
        "#         runs = batting_action_runs_map[batting_action]*np.random.choice(2,1,p=[1-prun,prun])\n",
        "#       return wicket, runs"
      ],
      "metadata": {
        "id": "GazuTGydVYAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pout_actions_min = np.array([0.005,0.01,0.02,0.1,0.2,0.3])\n",
        "\n",
        "pout_actions_max = np.array([0.05,0.1,0.15,0.25,0.35,0.0])\n",
        "prun_features_max = 0.65\n",
        "prun_features_min = 0.25\n",
        "\n",
        "risk_max_aggression = 1.2\n",
        "risk_min_aggression = 1.05\n",
        "risk_max_economical = 0.8\n",
        "risk_min_economical = 0.5\n",
        "\n",
        "easy_max_aggression = 0.2\n",
        "easy_min_aggression = 0.3\n",
        "easy_max_economical = -0.2\n",
        "easy_min_economical = -0.05\n",
        "\n",
        "batting_action_runs_map = np.array([0,1,2,3,4,6])\n",
        "bowling_action_risk_map = np.array([-0.5,0,1])\n",
        "\n",
        "\n",
        "class Match:\n",
        "\n",
        "  def __init__(self,num_balls, explore_num_balls,action_timeout, explore_timeout, TeamOne, TeamTwo):\n",
        "    # self.environment = Environment()\n",
        "    self.num_balls = num_balls\n",
        "    self.explore_num_balls = explore_num_balls\n",
        "    self.action_timeout = action_timeout\n",
        "    self.explore_timeout = explore_timeout\n",
        "    self.team_one_features = np.random.uniform(1,5,size=(5,4))   # team one feature\n",
        "    self.team_two_features = np.random.uniform(1,5,size=(5,4))\n",
        "    self.team_one = TeamOne(self.team_one_features, self.team_two_features)  # team features are initialsed\n",
        "    self.team_two = TeamTwo(self.team_two_features, self.team_one_features)\n",
        "    self.current_batters_list =np.array([1,1,1,1,1])\n",
        "    self.current_bowlers_list =np.array([2,2,2,2,2])\n",
        "    self.num_miss_team_batting = 0\n",
        "    self.num_miss_team_bowling = 0\n",
        "    self.order = dict.fromkeys([0,1,2,3,4],0)\n",
        "\n",
        "\n",
        "  def explore_phase_team(self,team_id):\n",
        "    if (team_id == 1):\n",
        "      team = self.team_one\n",
        "    else:\n",
        "      team = self.team_two\n",
        "    start_time = time.time()\n",
        "\n",
        "    for balls in range(self.explore_num_balls):\n",
        "      feature_batter,feature_bowler, batting_action, bowling_action,batter,bowler = team.get_explore_actions()\n",
        "      wicket, runs = self.get_outcome(feature_batter, feature_bowler, batting_action, bowling_action)\n",
        "\n",
        "      max_balls = 60\n",
        "      max_wickets = 5\n",
        "      max_v = 0\n",
        "      V=np.zeros((max_balls,max_wickets),dtype=float)\n",
        "      Q=np.zeros((max_balls,max_wickets,5),dtype=float)\n",
        "      V[max_balls-1,:] = self.Calculate_Value(np.zeros((max_wickets),dtype=float), feature_batter, feature_bowler)\n",
        "      for i in range(max_balls-2,-1,-1):\n",
        "        V[i,:]=self.Calculate_Value(V[i+1,:], feature_batter, feature_bowler)\n",
        "      self.order[batter] += sum(V)/300\n",
        "\n",
        "\n",
        "      team.set_explore_outcomes(wicket,runs)\n",
        "      team.explore_compute()\n",
        "\n",
        "    team.explore_dp()\n",
        "    end_time = time.time()\n",
        "    if(end_time - start_time > self.explore_timeout):\n",
        "      print(\"Timing Violation During Exploration Phase\")\n",
        "    return self.order\n",
        "\n",
        "\n",
        "  def explore_phase(self):\n",
        "    order1 = self.explore_phase_team(1)\n",
        "    order2 = self.explore_phase_team(2)\n",
        "    #print(order1, order2)\n",
        "    return order1,order2\n",
        "\n",
        "\n",
        "  def get_valid_bowler(self,next_bowler):\n",
        "    if (self.current_bowlers_list[next_bowler]==0):\n",
        "      print(\"Bowler Invalid, Choosing Random Bowler\")\n",
        "      bowlers_with_overs_left = np.where(self.current_bowlers_list>0)[0]\n",
        "      next_bowler = np.random.choice(bowlers_with_overs_left)\n",
        "    return next_bowler\n",
        "\n",
        "\n",
        "  def get_valid_batter(self,next_batter):\n",
        "    if (self.current_batters_list[next_batter]==0):\n",
        "      print(\"Batter Invalid, Choosing Random Batter\")\n",
        "      batters_not_out = np.where(self.current_batters_list>0)[0]\n",
        "      next_batter = np.random.choice(batters_not_out)\n",
        "    return next_batter\n",
        "\n",
        "  def next_batter(self,team_batting, order,wicket):\n",
        "    next_batter = team_batting.get_next_batter(order,wicket)\n",
        "    next_batter = self.get_valid_batter(next_batter)\n",
        "    feature_batter = team_batting.self_features[next_batter,0:2]\n",
        "    return next_batter,feature_batter\n",
        "\n",
        "\n",
        "  def next_bowler(self,team_bowling):\n",
        "    next_bowler = team_bowling.get_next_bowler()\n",
        "    next_bowler = self.get_valid_bowler(next_bowler)\n",
        "    feature_bowler = team_bowling.self_features[next_bowler,2:4]\n",
        "    return next_bowler, feature_bowler\n",
        "\n",
        "  def get_team_batting_action(self,team_batting,ball,total_runs,wickets_left,runs_to_chase,feature_batter,feature_bowler):\n",
        "    start_time      = time. time()\n",
        "    batting_action  = team_batting.get_batting_action(ball,total_runs,wickets_left,runs_to_chase,feature_batter,feature_bowler)\n",
        "    end_time        = time. time()\n",
        "    if(end_time - start_time > self.action_timeout):\n",
        "      batting_action = 0 #this is the default option, we have to fix the penalisation strategy\n",
        "      self.num_miss_team_batting = self.num_miss_team_batting + 1\n",
        "    return batting_action\n",
        "\n",
        "\n",
        "  def get_team_bowling_action(self,team_bowling,ball,total_runs,wickets_left,runs_to_chase,feature_batter,feature_bowler):\n",
        "    start_time      = time. time()\n",
        "    bowling_action  = team_bowling.get_bowling_action(ball,total_runs,wickets_left,runs_to_chase,feature_batter,feature_bowler)\n",
        "    end_time        = time. time()\n",
        "    if(end_time - start_time > self.action_timeout):\n",
        "      bowling_action = 0 #this is the default option, we have to fix the penalisation strategy\n",
        "      self.num_miss_team_bowling = self.num_miss_team_bowling + 1\n",
        "    return bowling_action\n",
        "\n",
        "\n",
        "  def get_outcome(self,feature_batter, feature_bowler, batting_action, bowling_action):\n",
        "    # wicket = np.random.randint(0,1)\n",
        "    # runs = np.random.randint(0,7)\n",
        "    runs = 0\n",
        "    pout = pout_actions_min[batting_action]*(1-((feature_batter[0]-1)/4))+((feature_batter[0]-1)/4)*pout_actions_max[batting_action]\n",
        "    risk = 1\n",
        "    if (bowling_action == 2):\n",
        "      risk = risk_max_aggression*(1-((feature_bowler[0]-1)/4))+((feature_bowler[0]-1)/4)*risk_min_aggression\n",
        "    if (bowling_action == 0):\n",
        "      risk = risk_max_economical*(1-((feature_bowler[0]-1)/4))+((feature_bowler[0]-1)/4)*risk_min_economical\n",
        "    pout = pout*risk\n",
        "    wicket = np.random.choice(2,1,p=[1-pout,pout])\n",
        "    wicket = wicket[0]\n",
        "    if (wicket == 0):\n",
        "      prun = prun_features_max*(1-((feature_batter[1]-1)/4))+((feature_batter[1]-1)/4)*prun_features_min\n",
        "      easy = 0\n",
        "      if (bowling_action == 2):\n",
        "        easy = easy_max_aggression*(1-((feature_bowler[1]-1)/4))+((feature_bowler[1]-1)/4)*easy_min_aggression\n",
        "      if (bowling_action == 0):\n",
        "        easy = easy_max_economical*(1-((feature_bowler[1]-1)/4))+((feature_bowler[1]-1)/4)*easy_min_economical\n",
        "      # print(prun, feature_batter[1],easy)\n",
        "      prun = prun + easy\n",
        "      runs = batting_action_runs_map[batting_action]*np.random.choice(2,1,p=[1-prun,prun])\n",
        "    else:\n",
        "      #  runs = 0*np.random.choice(2,1,p=[0.5,0.5])\n",
        "      runs = 0\n",
        "\n",
        "    return wicket, runs\n",
        "\n",
        "\n",
        "  def Calculate_Value(self, V_in, feature_batter, feature_bowler):\n",
        "    #mcts\n",
        "    runs=[0,1,2,3,4,6]\n",
        "    max_wickets = 5\n",
        "    Q_out=np.zeros((max_wickets,6),dtype=float)\n",
        "    V_out=np.zeros(np.size(V_in),dtype=float) # max wickets size\n",
        "    shot=np.zeros(np.size(V_in),dtype=int)\n",
        "\n",
        "    for i in range(1,np.size(V_in)):\n",
        "      for a in range(np.size(runs)):\n",
        "        x = np.random.randint(0,3)\n",
        "        p_w = ((x+0.05)*(a+0.05)*feature_batter[0])/(feature_bowler[0]+62) #self.p_out(feature_batter[0], feature_bowler[0])[x, a] # batting features, bowling features, batting_action, bowling_action ###\n",
        "        p_r = ((feature_bowler[1]+0.05)/((a+5)*feature_batter[1])) - 0.01  #self.p_run(feature_batter[1], feature_bowler[1])[x]\n",
        "        # a being runs\n",
        "        Q_out[i][a]=(1-p_w)*p_r*runs[a]+p_w*V_in[i-1]+(1-p_w)*V_in[i]\n",
        "      V_out[i]=np.max(Q_out[i,:])\n",
        "      shot[i]=runs[np.argmax(Q_out[i,:])]\n",
        "    return V_out\n",
        "\n",
        "\n",
        "  def innings(self,innigins_id, runs_to_chase, order, dqn_learning = True):\n",
        "\n",
        "    total_runs = 0\n",
        "    wickets_left = 5\n",
        "    self.current_batters_list = np.array([1,1,1,1,1])\n",
        "    self.current_bowlers_list = np.array([2,2,2,2,2])\n",
        "    self.num_miss_team_batting = 0\n",
        "    self.num_miss_team_bowling = 0\n",
        "    if (innigins_id == 1 ):\n",
        "      team_batting = self.team_one\n",
        "      team_bowling = self.team_two\n",
        "    else:\n",
        "      team_batting = self.team_two\n",
        "      team_bowling = self.team_one\n",
        "    # Initialising the first batter and first bowler\n",
        "    batter, feature_batter = self.next_batter(team_batting,order,5-wickets_left)\n",
        "    bowler, feature_bowler = self.next_bowler(team_bowling)\n",
        "\n",
        "    for ball in range(self.num_balls):\n",
        "      if np.sum(self.current_batters_list) > 0 :\n",
        "        batting_action = self.get_team_batting_action(team_batting, self.num_balls - ball, total_runs, wickets_left, runs_to_chase, feature_batter, feature_bowler)\n",
        "        bowling_action = self.get_team_bowling_action(team_bowling,ball,total_runs,wickets_left,runs_to_chase,feature_batter,feature_bowler)\n",
        "        wicket, runs   = self.get_outcome(feature_batter, feature_bowler, batting_action, bowling_action)\n",
        "        print(\"Batting Action:\", batting_action, \"\\t Bowling Action: \", bowling_action, \"\\t Runs:\", runs, \"\\t Wickets:\", wicket)\n",
        "        total_runs     = total_runs + runs\n",
        "        if (wicket > 0):\n",
        "          wickets_left = wickets_left - 1\n",
        "          self.current_batters_list[batter] = 0\n",
        "          if(np.sum(self.current_batters_list) > 0 ):\n",
        "            batter,feature_batter = self.next_batter(team_batting,order,wicket)\n",
        "        if ((ball+1)%6 ==0 ):\n",
        "          self.current_bowlers_list[bowler] = self.current_bowlers_list[bowler]-1\n",
        "          if(np.sum(self.current_bowlers_list) > 0 ) :\n",
        "            bowler, feature_bowler = self.next_bowler(team_bowling)\n",
        "        if(dqn_learning):\n",
        "          state = np.array([self.num_balls-ball+1, wickets_left+wicket])\n",
        "          next_state = np.array([[self.num_balls-ball, wickets_left]])\n",
        "          done = False\n",
        "          if(ball == self.num_balls - 1):\n",
        "            done = True\n",
        "          team_batting.agent.step(state, batting_action, runs, next_state, done)\n",
        "\n",
        "    return total_runs, wickets_left, self.current_batters_list, self.current_bowlers_list, self.num_miss_team_batting, self.num_miss_team_bowling\n",
        "\n",
        "  def dqn(self, n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    scores = []\n",
        "    ''' list containing scores from each episode '''\n",
        "\n",
        "    scores_window_printing = deque(maxlen=10)\n",
        "    ''' For printing in the graph '''\n",
        "\n",
        "    scores_window= deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        # state = env.reset()\n",
        "        score = 0\n",
        "        # for t in range(max_t):\n",
        "            # action = agent.act(state, eps)\n",
        "            # next_state, reward, done, _ = env.step(action)\n",
        "            # agent.step(state, action, reward, next_state, done)\n",
        "            # state = next_state\n",
        "            # if done:\n",
        "            #     break\n",
        "        total_runs, wickets_left, _, _, _, _ = self.innings(1, float('inf'), order1)\n",
        "        score += total_runs\n",
        "        print(\"--------------------------------------------------------------------\")\n",
        "\n",
        "        scores_window.append(score)\n",
        "        scores_window_printing.append(score)\n",
        "        ''' save most recent score '''\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        # print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        # if i_episode % 10 == 0:\n",
        "        #     scores.append(np.mean(scores_window_printing))\n",
        "        # if i_episode % 100 == 0:\n",
        "        #   print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        # if np.mean(scores_window)>= -110.0:\n",
        "        #   print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "        #   # break\n",
        "\n",
        "    return [np.array(scores),i_episode-100]\n"
      ],
      "metadata": {
        "id": "S6Wgoa3OeoDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting time outs\n",
        "explore_timeout = 100\n",
        "action_timeout = 100\n",
        "\n",
        "explore_num_balls = 10\n",
        "num_balls = 2500\n",
        "match = Match(num_balls,explore_num_balls,action_timeout, explore_timeout,Australia,India)\n",
        "order1,order2 = match.explore_phase()\n",
        "\n",
        "# mcts(runs = 0, wickets = 0, order)\n",
        "first_innings_score, wickets_left, batters_list, bowlers_list, num_miss_team_batting, num_miss_team_bowling = match.innings(1, float('inf'), order1)\n",
        "print(f\"Total runs = {first_innings_score}, Wickets left = {wickets_left}\")\n",
        "#second_innings_score, batters_list, bowlers_list, num_miss_team_batting, num_miss_team_bowling = match.innings(2,first_innings_score)"
      ],
      "metadata": {
        "id": "oTKOIy_WesCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting time outs\n",
        "explore_timeout = 100\n",
        "action_timeout = 100\n",
        "\n",
        "explore_num_balls = 10\n",
        "num_balls = 110\n",
        "match = Match(num_balls,explore_num_balls,action_timeout, explore_timeout,Australia,India)\n",
        "order1,order2 = match.explore_phase()\n",
        "\n",
        "# mcts(runs = 0, wickets = 0, order)\n",
        "match.dqn()\n",
        "print(f\"Total runs = {first_innings_score}, Wickets left = {wickets_left}\")\n",
        "#second_innings_score, batters_list, bowlers_list, num_miss_team_batting, num_miss_team_bowling = match.innings(2,first_innings_score)"
      ],
      "metadata": {
        "id": "TAAIdT0-q0JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4bpOzMFjCNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}